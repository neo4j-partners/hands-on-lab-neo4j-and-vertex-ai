{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d8979e",
   "metadata": {},
   "source": [
    "# Chatbot\n",
    "In this notebook, we'll build a chatbot.  The chatbot interface will use gradio.  Underlying the chatbot is the Neo4j knowledge graph we built in previous labs.  The chatbot uses generative AI and langchain.  We take a natural language question, convert it to Neo4j Cypher using generative AI and run that against the database.  We take the response from the database and use generative AI again to convert that back to natural language before presenting it to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea81aba-c3e5-435b-9a9a-86d0a65da844",
   "metadata": {},
   "source": [
    "## Base Example Without Grounding\n",
    "Before grounding with the Neo4j, let's setup up a baseline that just uses an LLM to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d45b7-1406-4454-8811-67a2b351c7ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAI\n",
    "\n",
    "base_chain = VertexAI(model_name='gemini-2.0-pro-001', max_output_tokens=2048, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e6d342-e9dc-4533-955e-601e181011ae",
   "metadata": {},
   "source": [
    "We can now ask a simple finance question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f933a2d9-53fd-41df-8b5a-248885b0dbda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_response = base_chain.invoke(\"\"\"What are the top 10 investments for Rempart?\"\"\")\n",
    "print(f\"Final answer: {base_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f64a21-8347-4eec-a83a-8044f824a3d8",
   "metadata": {},
   "source": [
    "While this answer may seem reasonable, we have no real way to know how the LLM came it with it, or where it was sourced from.\n",
    "\n",
    "Here is a more complicated example where we expect the LLM to understand some more specific terminology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43c396-39d5-4511-bc3d-a56a1880afc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_response = base_chain.invoke(\"\"\"Which managers own FAANG stocks?\"\"\")\n",
    "print(f\"Final answer: {base_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533cf10e-45f4-427d-bda6-a7ae4c706d92",
   "metadata": {},
   "source": [
    "In this case, it looks like the LLM understands the ubiquitous acronym FAANG but, unsurprisingly, the results indicate it doesn't understand manager within the context of our data model.  In your use case, you may have lots of specific terminology/ontology like this that you would need a chatbot to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a087cb5-f99d-4c7c-bdac-14ec8cd411cb",
   "metadata": {},
   "source": [
    "## Grounding LLMs with Neo4j\n",
    "Now let's create a chatbot that is grounded with Neo4j. Below is the pattern we will follow with LangChain:\n",
    "\n",
    "![](images/langchain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c044359a",
   "metadata": {},
   "source": [
    "## Cypher Generation\n",
    "We have to use a prompt template that: \n",
    "1. Clearly states what schema to use \n",
    "2. Provides principles the chatbot should follow in generating responses\n",
    "3. Demonstrates few-shot examples to help the chatbot be more accurate in its query generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e5c4a7-9821-4fe8-9b26-49dc96d89103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CYPHER_GENERATION_TEMPLATE = \"\"\"You are an expert Neo4j Cypher translator who understands the question in english and convert to Cypher strictly based on the Neo4j Schema provided and following the instructions below:\n",
    "1. Generate Cypher query compatible ONLY for Neo4j Version 5\n",
    "2. Do not use EXISTS, SIZE, CONTAINS ANY keywords in the cypher. Use alias when using the WITH keyword\n",
    "3. Please do not use same variable names for different nodes and relationships in the query.\n",
    "4. Use only Nodes and relationships mentioned in the schema\n",
    "5. Always enclose the Cypher output inside 3 backticks\n",
    "6. Always do a case-insensitive and fuzzy search for any properties related search. Eg: to search for a Company name use `toLower(c.name) contains 'neo4j'`\n",
    "7. Candidate node is synonymous to Manager\n",
    "8. Always use aliases to refer the node in the query\n",
    "9. 'Answer' is NOT a Cypher keyword. Answer should never be used in a query.\n",
    "10. Please generate only one Cypher query per question. \n",
    "11. Cypher is NOT SQL. So, do not mix and match the syntaxes.\n",
    "12. Every Cypher query always starts with a MATCH keyword.\n",
    "13. Always use `IN` keyword instead of `CONTAINS ANY`\n",
    "\n",
    "Schema:\n",
    "{schema}\n",
    "Samples:\n",
    "Question: Which fund manager owns most shares? What is the total portfolio value?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) RETURN m.managerName as manager, sum(distinct o.shares) as ownedShares, sum(o.value) as portfolioValue ORDER BY ownedShares DESC LIMIT 10\n",
    "\n",
    "Question: Which fund manager owns most companies? How many shares?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) RETURN m.managerName as manager, count(distinct c) as ownedCompanies, sum(distinct o.shares) as ownedShares ORDER BY ownedCompanies DESC LIMIT 10\n",
    "\n",
    "Question: What are the top 10 investments for Vanguard?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(m.managerName) contains \"vanguard\" RETURN c.companyName as Investment, sum(DISTINCT o.shares) as totalShares, sum(DISTINCT o.value) as investmentValue order by investmentValue desc limit 10\n",
    "\n",
    "Question: What other fund managers are investing in same companies as Vanguard?\n",
    "Answer: MATCH (m1:Manager) -[:OWNS]-> (c1:Company) <-[o:OWNS]- (m2:Manager) WHERE toLower(m1.managerName) contains \"vanguard\" AND elementId(m1) <> elementId(m2) RETURN m2.managerName as manager, sum(DISTINCT o.shares) as investedShares, sum(DISTINCT o.value) as investmentValue ORDER BY investmentValue LIMIT 10\n",
    "\n",
    "Question: What are the top investors for Apple?\n",
    "Answer: MATCH (m1:Manager) -[o:OWNS]-> (c1:Company) WHERE toLower(c1.companyName) contains \"apple\" RETURN distinct m1.managerName as manager, sum(o.value) as totalInvested ORDER BY totalInvested DESC LIMIT 10\n",
    "\n",
    "Question: Which managers own FAANG stocks?\n",
    "Answer: MATCH (m:Manager)-[o:OWNS]->(c:Company) WHERE toLower(c.companyName) IN [\"facebook\", \"amazon\", \"apple\", \"netflix\", \"google\"] RETURN m.managerName AS manager, c.companyName AS company\n",
    "\n",
    "Question: What are the other top investments for fund managers investing in Apple?\n",
    "Answer: MATCH (c1:Company) <-[:OWNS]- (m1:Manager) -[o:OWNS]-> (c2:Company) WHERE toLower(c1.companyName) contains \"apple\" AND elementId(c1) <> elementId(c2) RETURN DISTINCT c2.companyName as company, sum(DISTINCT o.value) as totalInvested, sum(DISTINCT o.shares) as totalShares ORDER BY totalInvested DESC LIMIT 10\n",
    "\n",
    "Question: What are the top investors in the last 3 months?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE date() > o.reportCalendarOrQuarter > o.reportCalendarOrQuarter - duration({{months:3}}) RETURN distinct m.managerName as manager, sum(o.value) as totalInvested, sum(o.shares) as totalShares ORDER BY totalInvested DESC LIMIT 10\n",
    "\n",
    "Question: Which managers own FAANG stocks?\n",
    "Answer: MATCH (m:Manager)-[o:OWNS]->(c:Company) WHERE toLower(c.companyName) IN [\"facebook\", \"amazon\", \"apple\", \"netflix\", \"google\"] RETURN m.managerName AS manager, c.companyName AS company\n",
    "\n",
    "Question: What are top investments in last 6 months for Vanguard?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(m.managerName) contains \"vanguard\" AND date() > o.reportCalendarOrQuarter > date() - duration({{months:6}}) RETURN distinct c.companyName as company, sum(o.value) as totalInvested, sum(o.shares) as totalShares ORDER BY totalInvested DESC LIMIT 10\n",
    "\n",
    "Question: Who are Apple's top investors in last 3 months?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(c.companyName) contains \"apple\" AND date() > o.reportCalendarOrQuarter > date() - duration({{months:3}}) RETURN distinct m.managerName as investor, sum(o.value) as totalInvested, sum(o.shares) as totalShares ORDER BY totalInvested DESC LIMIT 10\n",
    "\n",
    "Question: Which fund manager under 200 million has similar investment strategy as Vanguard?\n",
    "Answer: MATCH (m1:Manager) -[o1:OWNS]-> (:Company) <-[o2:OWNS]- (m2:Manager) WHERE toLower(m1.managerName) CONTAINS \"vanguard\" AND elementId(m1) <> elementId(m2) WITH distinct m2 AS m2, sum(distinct o2.value) AS totalVal WHERE totalVal < 200000000 RETURN m2.managerName AS manager, totalVal*0.000001 AS totalVal ORDER BY totalVal DESC LIMIT 10\n",
    "\n",
    "Question: Who are common investors in Apple and Amazon?\n",
    "Answer: MATCH (c1:Company) <-[:OWNS]- (m:Manager) -[:OWNS]-> (c2:Company) WHERE toLower(c1.companyName) contains \"apple\" AND toLower(c2.companyName) CONTAINS \"amazon\" RETURN DISTINCT m.managerName LIMIT 50\n",
    "\n",
    "Question: What are Vanguard's top investments by shares for 2023?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(m.managerName) CONTAINS \"vanguard\" AND date({{year:2023}}) = date.truncate('year',o.reportCalendarOrQuarter) RETURN c.companyName AS investment, sum(o.value) AS totalValue ORDER BY totalValue DESC LIMIT 10\n",
    "\n",
    "Question: What are Vanguard's top investments by value for 2023?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(m.managerName) CONTAINS \"vanguard\" AND date({{year:2023}}) = date.truncate('year',o.reportCalendarOrQuarter) RETURN c.companyName AS investment, sum(o.shares) AS totalShares ORDER BY totalShares DESC LIMIT 10\n",
    "\n",
    "Question: {question}\n",
    "Answer: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e46b3",
   "metadata": {},
   "source": [
    "Now let’s create a LangChain prompt template.  \n",
    "\n",
    "This template defines the parameter inputs for the prompt sent to the Cypher generation bot.  In our example, the inputs will be schema and question.  The question comes from the end user.  The LangChain GraphCypherQAChain automatically inserts the schema via a built-in method to Neo4jGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86addda3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=['schema','question'], validate_template=True, template=CYPHER_GENERATION_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137ff3ef",
   "metadata": {},
   "source": [
    "Now we'll load up the Aura credentials from the credential file we created in Lab 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493b700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "dotenv_file = \"../aura_connection.txt\"\n",
    "load_dotenv(dotenv_file)\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USERNAME = os.getenv(\"NEO4J_USERNAME\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "print('NEO4J_URI:', NEO4J_URI)\n",
    "print('NEO4J_USERNAME:', NEO4J_USERNAME)\n",
    "print('NEO4J_PASSWORD:', NEO4J_PASSWORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180e2e7d",
   "metadata": {},
   "source": [
    "We need to connect to the graph via LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbc48f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=NEO4J_URI, \n",
    "    username=NEO4J_USERNAME, \n",
    "    password=NEO4J_PASSWORD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1263e2",
   "metadata": {},
   "source": [
    "We define our `chain` object (specifically a`GraphCypherQAChain`) using two vertex AI LLMs:\n",
    "* [gemini-2.0-flash](https://ai.google.dev/gemini-api/docs/models/gemini#gemini-2.0-flash) to translate user questions to Cypher queries\n",
    "* [gemini-1.5-pro](https://ai.google.dev/gemini-api/docs/models/gemini#gemini-1.5-pro)  to convert Cypher query results back to natural language for human-friendly responses. \n",
    "\n",
    "`GraphCypherQAChain` also takes a ‘Neo4jGraph’ so it can handle the chatbot process end-to-end, from taking the user question and translating to Cypher to executing the query, getting results, translating back to natural language, and returning to the user. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f94866b-7729-46a5-85cb-c11fb364eefa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "import json\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    llm=VertexAI(model_name='gemini-2.0-flash-001', max_output_tokens=2048, temperature=0.0),\n",
    "    graph=graph,\n",
    "    cypher_prompt=CYPHER_GENERATION_PROMPT,\n",
    "    verbose=True,\n",
    "    return_direct=True,\n",
    "    allow_dangerous_requests=True,\n",
    "    allowed_operations=['GET']\n",
    ")\n",
    "\n",
    "def chat(que):\n",
    "    r = chain.invoke(que)\n",
    "    print(r)\n",
    "    llm=VertexAI(model_name='gemini-2.0-flash-001', max_output_tokens=2048, temperature=0.0)\n",
    "    summary_prompt_tpl = f\"\"\"Human: \n",
    "    Fact: {json.dumps(r['result'])}\n",
    "\n",
    "    * Summarise the above fact as if you are answering this question \"{r['query']}\"\n",
    "    * When the result inside fact is not empty, assume the question is valid and the answer is true\n",
    "    * Do not return helpful or extra text or apologies\n",
    "    * Just return summary to the user. DO NOT start with Here is a summary\n",
    "    * List the result in rich text format if there are more than one results\n",
    "    * If there is an empty result inside Facts, do not try to provide your own answer\n",
    "    Assistant:\n",
    "    \"\"\"\n",
    "    return llm.invoke(summary_prompt_tpl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d1590f",
   "metadata": {},
   "source": [
    "Below we have a few examples of how we can get answers from the chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed32736-43f8-4c9c-b90e-69932461bbd3",
   "metadata": {},
   "source": [
    "## Why Ground Your LLM?\n",
    "Recall our base example where we asked for the top 10 Rempart investments?  We got an answer that looked like it may be reasonable, but we couldn't validate it or track sources.  We also asked what managers own FAANG stocks, and for that, we unsurprisingly received the wrong answers for our use case.\n",
    "\n",
    "Let's try again grounding with Neo4j. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9e6bf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r2 = chat(\"\"\"What are the top 10 investments for Rempart?\"\"\")\n",
    "print(f\"Final answer: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65434f3-f818-46ca-9fc8-0daa4a77942b",
   "metadata": {},
   "source": [
    "Notice that this answer is different from our base example, and this time we have the Cypher logic used to obtain the answer from our database. This means that we can trace back how we came up with this answer and make any adjustments to our database or prompt if we need to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7da574-8c23-4c78-b35f-bb4e86cdb291",
   "metadata": {},
   "source": [
    "Now lets try the FAANG question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d1464-7202-4d66-af52-3a6ffe8668d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r3 = chat(\"\"\"Which managers own FAANG stocks?\"\"\")\n",
    "print(f\"Final answer: {r3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0686244-bb6b-4231-aebf-70b3c777ef25",
   "metadata": {},
   "source": [
    "Here again, we notice the traceability with Cypher, and because we engineered our prompt to include our schema, it understood what “manager” means in the context of our use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d119d1-5b84-4665-985c-42eeeca49779",
   "metadata": {},
   "source": [
    "## Why Ground your LLM with Neo4j?\n",
    "There are 3 primary reasons to ground your LLM with Neo4j specifically:\n",
    "1. __Grounding for more complex question handling__: Multi-hop knowledge retrieval across connected data. Connections between data points are calculated before query time. \n",
    "2. __Enterprise reliability and security__: Fine-grained security so the chatbot only accesses information the user has permission to. Autonomous clustering for horizontal scaling.  Fully managed service in the cloud through Aura. \n",
    "3. __Performance__: fast queries with high concurrency for many users.\n",
    "\n",
    "We can explore point 1 with more complex questions below.\n",
    "\n",
    "A question requiring ~4 hops (would be joins in the relational world).  Having a knowledge graph with relationships calculated before query time allows us to answer the question quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a980deb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r4 = chat(\"\"\"What are the other top investments for fund managers investing in Lilly?\"\"\")\n",
    "print(f\"Final answer: {r4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c1f42-d475-4075-9168-61f65fe6284f",
   "metadata": {},
   "source": [
    "Combine also with property conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877ae99c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r5 = chat(\"\"\"Which fund managers under 200 million have the most similar investment strategies to Rempart? Return the top 10.\"\"\")\n",
    "print(f\"Final answer: {r5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbca6ab-01d6-4697-a58f-7978567ff66b",
   "metadata": {},
   "source": [
    "and more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588955da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r6 = chat(\"\"\"Please get me common investors between Tesla and Costco\"\"\")\n",
    "print(f\"Final answer: {r6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8b038e",
   "metadata": {},
   "source": [
    "## Grounded Chatbot\n",
    "Now we will use Gradio to deploy a chat interface with our chain behind it.\n",
    "\n",
    "The below code deploys a Gradio application.  You can access the app via a local URL. A publicly sharable URL is also provided (sharable for 3 days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe1766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import typing_extensions\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key = \"chat_history\", return_messages = True)\n",
    "\n",
    "def chat_response(input_text,history):\n",
    "\n",
    "    try:\n",
    "        return chat(input_text)\n",
    "    except:\n",
    "        # a bit of protection against exposed error messages\n",
    "        # we could log these situations in the backend to revisit later in development\n",
    "        return \"I'm sorry, there was an error retrieving the information you requested.\"\n",
    "\n",
    "interface = gr.ChatInterface(fn = chat_response,\n",
    "                             title = \"Investment Chatbot\",\n",
    "                             description = \"powered by Neo4j\",\n",
    "                             theme = \"soft\",\n",
    "                             chatbot = gr.Chatbot(height=500),\n",
    "                             #undo_btn = None,\n",
    "                             #clear_btn = \"\\U0001F5D1 Clear chat\",\n",
    "                             examples = [\"What are the top 10 investments for Rempart?\",\n",
    "                                         \"Which manager owns FAANG stocks?\",\n",
    "                                         \"What are other top investments for fund managers investing in Exxon?\",\n",
    "                                         \"What are Rempart's top investments by value for 2021?\",\n",
    "                                         \"Who are the common investors between Tesla and Costco?\",\n",
    "                                         \"Who are Tesla's top investors in last 48 months?\"])\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7c8755-2489-4a1f-8508-ed4b519fe43b",
   "metadata": {},
   "source": [
    "## Semantic Layers\n",
    "The Text to Cypher approach we just did above can be brittle due to the indeterministic nature of models. For Production, you might need to be more deterministic and robust. [Semantic Layers approach](https://github.com/neo4j-partners/neo4j-generative-ai-google-cloud/blob/main/assetmanager/ui/streamlit/semantic_layer/semantic_fn.py) is more suitable here. It leverages on Tooling or Function-calling capabilities of LLMs. So, user queries can be directed to relevant templated cypher queries via function-calling. This way, no Cypher gets geenrated by the LLM. Instead, user's intent and query parameters are captured and directed to the relevant templated cypher code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00e27bc-e586-467b-8ace-6d3893d2fad1",
   "metadata": {},
   "source": [
    "## Fine Tuning for Cypher Generation\n",
    "We encourage you to use Vertex AI Gemini family models with a schema, few-shot examples, and precise prompt engineering for Cypher generation. However, if that still doesn't provide an appropriate level of quality, or you need your LLM to improve accuracy on a more specific task area, you can try fine-tuning.\n",
    "\n",
    "Fine-tuning is the process of taking a foundational model and making precise changes to improve its performance for a specific, narrower task. It works by taking in training data containing many examples of your specific task and using it to update or add additional parameters in a new version of the model.\n",
    "\n",
    "The total training time generally takes more than an hour. The tuned adapter model is going to stay within your tenant, and your training data will not be used to train the base model, which is frozen. Tuning runs on GCP's TPU infrastructure that is optimized to run ML workloads.\n",
    "\n",
    "The training data should be structured as a supervised training set, where each row contains input text and desired, resulting, Cypher query. Vertex AI expects you to adhere to the below format in a `jsonl` file.\n",
    "\n",
    "```\n",
    "{\"input_text\": \"MY_INPUT_PROMPT\", \"output_text\": \"CYPHER_QUERY\"}\n",
    "```\n",
    "You can find more about fine-tuning in the [Vertex AI documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5197afd",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we went through the steps of connecting a LangChain agent to a Neo4j database and using it to generate Cypher queries in response to user requests via LLMs on Vertex AI, thus grounding the LLM with a knowledge graph.\n",
    "\n",
    "While we used the `gemini-1.5-flash` and `gemini-1.5-pro` models here, this approach can be generalized to other Vertex AI LLMs.  This process can also be augmented with additional steps around the generation chain to customize the experience for specific use cases.  \n",
    "\n",
    "The critical takeaway is the importance of Neo4j Knowledge Graph as a grounding database to: \n",
    "* Anchor your chatbot to reality as it generates responses and \n",
    "* Enable your LLM to provide answers enriched with relevant enterprise data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1c3837-5cd9-4f10-9b0d-7869f7044b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
